{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo CNN for Wildfire Growth Prediction\n",
    "\n",
    "Below is starter code for a cnn solution to solve the wildfire growth challenge!\n",
    "\n",
    "We provide infrastructure and helper functions to call and process the data.\n",
    "\n",
    "It is up to your team to fill in necessary blanks and improve the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paths for data and fires\n",
    "data_path = \"/home/p/Downloads/wildfire-hackathon-kaggle/Wildfire_Hackathon_Complete/\"\n",
    "train_path = data_path + \"Train/\"\n",
    "test_path = data_path + \"Test/\"\n",
    "tr_fnums = [\"fire1209\", \"fire1298\", \"fire1386\", \"fire2034\", \"fire2210\", \"fire2211\", \"fire2212\"]\n",
    "te_fnums = [\"fire2214\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util variables\n",
    "device = 'cuda'\n",
    "target_shape = (528, 720)\n",
    "\n",
    "# Util functions\n",
    "def pad_to_fit(d, shape):\n",
    "    h, w = d.shape\n",
    "    pad_h = shape[0] - h\n",
    "    pad_w = shape[1] - w\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        d = np.pad(d, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return d\n",
    "\n",
    "def normalize(d):\n",
    "    m = np.mean(d)\n",
    "    s = np.std(d)\n",
    "    return (d - m)/s\n",
    "\n",
    "def tif2np(tif):\n",
    "    with rio.open(tif) as src:\n",
    "        data = src.read(1)  # Read the first band\n",
    "    return pad_to_fit(np.nan_to_num(data, nan=0.0), target_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to load data\n",
    "\n",
    "The load fire function loads and processes data for the denoted fire. The fire is then stacked into a numpy array.\n",
    "\n",
    "The load day function loads in a day of data for a specified fire. \n",
    "\n",
    "<ins>**Additional data should be loaded and specified into this function**.<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_day(path, day):\n",
    "    # fire_weather\n",
    "    fwi = path+'/fire_weather/fire_weather_index_day{}.tif'.format(day)\n",
    "    fwi = normalize(tif2np(fwi))\n",
    "    # weather relative humidity\n",
    "    wrh = path+'/weather/noon_relative_humidity_day{}.tif'.format(day)\n",
    "    wrh = normalize(tif2np(wrh))\n",
    "    # weather wind speed\n",
    "    wws = path+'/weather/noon_wind_speed_day{}.tif'.format(day)\n",
    "    wws = normalize(tif2np(wws))\n",
    "    # Add more data here\n",
    "    #...\n",
    "    return [fwi, wrh, wws]\n",
    "\n",
    "def load_fire(fire_num, split = \"Train\"):\n",
    "    path = train_path + fire_num\n",
    "    if split == \"Test\":\n",
    "        path = test_path + fire_num\n",
    "    \n",
    "    ftif = path + \"/fire/{}.tif\".format(fire_num)\n",
    "    if split == \"Test\":\n",
    "        ftif = path + \"/fire/{}_train.tif\".format(fire_num)\n",
    "    fdata = tif2np(ftif)\n",
    "\n",
    "    minjd, maxjd = int(np.min(fdata[np.nonzero(fdata)])), int(np.max(fdata))\n",
    "    lastjd = maxjd\n",
    "    if split == \"Test\":\n",
    "        maxjd += 21\n",
    "    \n",
    "    elev = normalize(tif2np(path+'/topography/dem.tif'))\n",
    "    slope = normalize(tif2np(path+'/topography/slope.tif'))\n",
    "    fuels = tif2np(path+'/fuels/fbp_fuels.tif')\n",
    "    ignition = tif2np(path+'/fire/ignitions.tif')\n",
    "\n",
    "    dataset = []\n",
    "    gt = ignition\n",
    "    cfire = ignition\n",
    "    for d in range(minjd, maxjd):\n",
    "        data = {}\n",
    "\n",
    "        fuels[cfire != 0] = 0\n",
    "        ft = [fuels]\n",
    "        ft.extend([cfire, gt, slope, elev])\n",
    "        ft.extend(load_day(path, d))\n",
    "        ft = np.stack(ft)\n",
    "        data['ft'] = ft\n",
    "\n",
    "        if d < lastjd:\n",
    "            gt = fdata == float(d)\n",
    "            data['gt'] = gt\n",
    "\n",
    "        cfire = np.logical_or(cfire ,gt)\n",
    "        \n",
    "        dataset.append(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets and dataloaders\n",
    "\n",
    "<ins>Create/implement data augmentations/transformations here<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "class FireDataset(Dataset):\n",
    "    def __init__(self, split=\"Train\"):\n",
    "        fnums = tr_fnums if split==\"Train\" else te_fnums\n",
    "        self.dataset = []\n",
    "        for fnum in fnums:\n",
    "            self.dataset.extend(load_fire(fnum, split=split))\n",
    "        print(len(self.dataset))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "trainset = FireDataset(split=\"Train\")\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [0.9,0.1])\n",
    "testset = FireDataset(split=\"Test\")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=8, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network/model\n",
    "\n",
    "In this example, we define a simple 2 layer cnn model. \n",
    "\n",
    "<ins>**Modify the model as you see fit!**<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FuelEmbeddings(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(FuelEmbeddings, self).__init__()\n",
    "\n",
    "        unique_values = [0, 1, 2, 3, 4, 7, 13, 31, 101, 425, 635, 650, 665]\n",
    "        self.unique_values = torch.tensor(unique_values).to(device)  # Unique values in the categorical feature\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(unique_values), embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, categorical_feature):\n",
    "        # (B,H,W) -> (B,H,W,U) wher U is unique values count\n",
    "        mask = categorical_feature.unsqueeze(-1) == self.unique_values\n",
    "        matching_indices = torch.argmax(mask.int(), dim=-1)\n",
    "\n",
    "        # Apply embedding and reshape\n",
    "        # (B,H,W,U) -> (B,H,W,6) -> (B,6,H,W) in default setting\n",
    "        embedded_fuel = self.embedding(matching_indices)\n",
    "        embedded_reshaped_fuel = embedded_fuel.permute(0, 3, 1, 2)\n",
    "\n",
    "        return embedded_reshaped_fuel\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "    def __init__(self, embedding_dim=6, num_features=8):\n",
    "        super(CNN1, self).__init__()\n",
    "\n",
    "        self.fuelembedding = FuelEmbeddings(embedding_dim)\n",
    "\n",
    "        # (266, 433)\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=(embedding_dim+num_features-1), out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=8, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        categorical_feature = x[:, 0, :, :]  # Extract the categorical feature\n",
    "        embedded_fuel = self.fuelembedding(categorical_feature)  # Transform the categorical feature\n",
    "\n",
    "        # Replace the original categorical feature with the embedded feature\n",
    "        x = torch.cat((embedded_fuel, x[:, 1:, :, :]), dim=1)\n",
    "\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        out = self.sigmoid(x)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss function\n",
    "\n",
    "<ins>**Create/define/specify your own loss function here!**<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        # threshold condition is not differentiable so just use softmaxed data\n",
    "        # Flatten the tensors\n",
    "        outputs = outputs.view(-1)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Compute the intersection\n",
    "        intersection = (outputs * labels).sum()\n",
    "\n",
    "        # Compute the union\n",
    "        union = outputs.sum() + labels.sum() - intersection\n",
    "        iou = intersection / (union + 1e-6)  # Add a small epsilon for numerical stability\n",
    "        loss = 1 - iou\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    total_steps = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        ft = batch['ft'].to(device).float()\n",
    "        gt = batch['gt'].to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(ft).squeeze()\n",
    "\n",
    "        loss = criterion(output, gt)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        total_steps += 1\n",
    "    return running_loss/total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    acc = []\n",
    "    iou = []\n",
    "    f1 = []\n",
    "    total_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            ft = batch['ft'].to(device)\n",
    "            gt = torch.flatten(batch['gt'])\n",
    "\n",
    "            output = torch.flatten(model(ft)).squeeze().cpu()\n",
    "            output = (output > 0.5)\n",
    "\n",
    "            acc.append(accuracy_score(gt, output))\n",
    "            iou.append(jaccard_score(gt, output))\n",
    "            f1.append(f1_score(gt, output))\n",
    "            total_steps += 1\n",
    "    return sum(acc)/total_steps, sum(iou)/total_steps, sum(f1)/total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference function\n",
    "\n",
    "Saves the inference results to a submission file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cfire = torch.zeros(target_shape)\n",
    "        for i, day in enumerate(dataloader):\n",
    "            ft = day['ft'].to(device)\n",
    "\n",
    "            # Create the submission file after 10 days\n",
    "            if i > 9:\n",
    "                cfire = torch.logical_or(output, cfire) # define the cumulative fire\n",
    "                ft[0][1] = cfire # set the cumulative fire for the next input\n",
    "                ft[0][2] = output # set the next step fire for the next input\n",
    "            else:\n",
    "                cfire = ft[0][1]\n",
    "\n",
    "            output = model(ft)\n",
    "            output = (output > 0.5)\n",
    "    \n",
    "    # Save the cumulative fire\n",
    "    pred = cfire.cpu().squeeze().numpy()\n",
    "    save_df = pd.DataFrame(pred)  # convert img data to df\n",
    "    save_df.to_csv(\"./output/submission.csv\", index_label='row')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training/eval/inference loop\n",
    "\n",
    "<ins>**Define new optimizers here**<ins>\n",
    "\n",
    "<ins>**Utilize a scheduler here**<ins>\n",
    "\n",
    "<ins>**Change the learning rate here**<ins>\n",
    "\n",
    "<ins>**Implement a better early stopping strategy here**<ins>\n",
    "\n",
    "<ins>**Implement other tricks here (i.e. EMA)**<ins>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0*  avg iou loss:0.994 avg acc: 0.496 avg f1: 0.015 avg iou: 0.008\n",
      "1*  avg iou loss:0.993 avg acc: 0.592 avg f1: 0.018 avg iou: 0.009\n",
      "2*  avg iou loss:0.992 avg acc: 0.673 avg f1: 0.022 avg iou: 0.011\n",
      "3*  avg iou loss:0.991 avg acc: 0.797 avg f1: 0.027 avg iou: 0.014\n",
      "4*  avg iou loss:0.989 avg acc: 0.930 avg f1: 0.067 avg iou: 0.035\n",
      "5*  avg iou loss:0.987 avg acc: 0.984 avg f1: 0.185 avg iou: 0.102\n",
      "6*  avg iou loss:0.968 avg acc: 0.984 avg f1: 0.193 avg iou: 0.107\n",
      "7*  avg iou loss:0.924 avg acc: 0.994 avg f1: 0.325 avg iou: 0.195\n",
      "8*  avg iou loss:0.864 avg acc: 0.994 avg f1: 0.376 avg iou: 0.233\n",
      "9*  avg iou loss:0.840 avg acc: 0.994 avg f1: 0.390 avg iou: 0.242\n",
      "10  avg iou loss:0.836 avg acc: 0.994 avg f1: 0.376 avg iou: 0.232\n",
      "11  avg iou loss:0.834 avg acc: 0.994 avg f1: 0.368 avg iou: 0.227\n",
      "12*  avg iou loss:0.832 avg acc: 0.994 avg f1: 0.400 avg iou: 0.250\n",
      "13  avg iou loss:0.827 avg acc: 0.994 avg f1: 0.392 avg iou: 0.246\n",
      "14  avg iou loss:0.822 avg acc: 0.994 avg f1: 0.398 avg iou: 0.250\n",
      "15*  avg iou loss:0.816 avg acc: 0.994 avg f1: 0.401 avg iou: 0.251\n",
      "16*  avg iou loss:0.823 avg acc: 0.995 avg f1: 0.414 avg iou: 0.262\n",
      "17  avg iou loss:0.834 avg acc: 0.994 avg f1: 0.407 avg iou: 0.256\n",
      "18*  avg iou loss:0.821 avg acc: 0.994 avg f1: 0.419 avg iou: 0.265\n",
      "19  avg iou loss:0.822 avg acc: 0.995 avg f1: 0.414 avg iou: 0.261\n",
      "20*  avg iou loss:0.815 avg acc: 0.994 avg f1: 0.431 avg iou: 0.276\n",
      "21  avg iou loss:0.823 avg acc: 0.995 avg f1: 0.417 avg iou: 0.263\n",
      "22  avg iou loss:0.820 avg acc: 0.994 avg f1: 0.403 avg iou: 0.253\n",
      "23  avg iou loss:0.812 avg acc: 0.994 avg f1: 0.420 avg iou: 0.266\n",
      "24  avg iou loss:0.816 avg acc: 0.995 avg f1: 0.385 avg iou: 0.241\n",
      "25  avg iou loss:0.811 avg acc: 0.994 avg f1: 0.382 avg iou: 0.240\n",
      "26  avg iou loss:0.815 avg acc: 0.994 avg f1: 0.411 avg iou: 0.259\n",
      "27*  avg iou loss:0.811 avg acc: 0.995 avg f1: 0.446 avg iou: 0.288\n",
      "28  avg iou loss:0.812 avg acc: 0.995 avg f1: 0.430 avg iou: 0.275\n",
      "29  avg iou loss:0.808 avg acc: 0.995 avg f1: 0.432 avg iou: 0.276\n",
      "30  avg iou loss:0.814 avg acc: 0.995 avg f1: 0.409 avg iou: 0.258\n",
      "31  avg iou loss:0.809 avg acc: 0.994 avg f1: 0.406 avg iou: 0.256\n",
      "32  avg iou loss:0.805 avg acc: 0.994 avg f1: 0.430 avg iou: 0.275\n",
      "33  avg iou loss:0.815 avg acc: 0.994 avg f1: 0.430 avg iou: 0.274\n",
      "34  avg iou loss:0.814 avg acc: 0.994 avg f1: 0.403 avg iou: 0.254\n",
      "35  avg iou loss:0.809 avg acc: 0.994 avg f1: 0.423 avg iou: 0.270\n",
      "36  avg iou loss:0.813 avg acc: 0.994 avg f1: 0.441 avg iou: 0.284\n",
      "37  avg iou loss:0.817 avg acc: 0.994 avg f1: 0.421 avg iou: 0.267\n",
      "38  avg iou loss:0.816 avg acc: 0.994 avg f1: 0.426 avg iou: 0.271\n",
      "39  avg iou loss:0.809 avg acc: 0.994 avg f1: 0.427 avg iou: 0.272\n",
      "40  avg iou loss:0.808 avg acc: 0.995 avg f1: 0.428 avg iou: 0.275\n",
      "41  avg iou loss:0.807 avg acc: 0.994 avg f1: 0.427 avg iou: 0.272\n",
      "42  avg iou loss:0.810 avg acc: 0.994 avg f1: 0.433 avg iou: 0.277\n",
      "43  avg iou loss:0.808 avg acc: 0.994 avg f1: 0.440 avg iou: 0.282\n",
      "44  avg iou loss:0.800 avg acc: 0.995 avg f1: 0.436 avg iou: 0.280\n",
      "45  avg iou loss:0.802 avg acc: 0.995 avg f1: 0.436 avg iou: 0.280\n",
      "46  avg iou loss:0.806 avg acc: 0.994 avg f1: 0.430 avg iou: 0.274\n",
      "47  avg iou loss:0.805 avg acc: 0.995 avg f1: 0.440 avg iou: 0.283\n",
      "48  avg iou loss:0.812 avg acc: 0.994 avg f1: 0.432 avg iou: 0.276\n",
      "49  avg iou loss:0.802 avg acc: 0.995 avg f1: 0.441 avg iou: 0.283\n",
      "50  avg iou loss:0.808 avg acc: 0.994 avg f1: 0.427 avg iou: 0.271\n",
      "51  avg iou loss:0.805 avg acc: 0.994 avg f1: 0.425 avg iou: 0.270\n",
      "52  avg iou loss:0.802 avg acc: 0.994 avg f1: 0.436 avg iou: 0.281\n",
      "53  avg iou loss:0.798 avg acc: 0.994 avg f1: 0.442 avg iou: 0.285\n",
      "54  avg iou loss:0.816 avg acc: 0.995 avg f1: 0.416 avg iou: 0.265\n",
      "55  avg iou loss:0.810 avg acc: 0.994 avg f1: 0.428 avg iou: 0.273\n",
      "56  avg iou loss:0.799 avg acc: 0.994 avg f1: 0.435 avg iou: 0.279\n",
      "57  avg iou loss:0.800 avg acc: 0.994 avg f1: 0.416 avg iou: 0.264\n",
      "58  avg iou loss:0.812 avg acc: 0.994 avg f1: 0.441 avg iou: 0.283\n",
      "59  avg iou loss:0.807 avg acc: 0.995 avg f1: 0.437 avg iou: 0.280\n",
      "60  avg iou loss:0.806 avg acc: 0.994 avg f1: 0.439 avg iou: 0.282\n",
      "61  avg iou loss:0.804 avg acc: 0.995 avg f1: 0.430 avg iou: 0.275\n",
      "62  avg iou loss:0.798 avg acc: 0.995 avg f1: 0.425 avg iou: 0.271\n",
      "63  avg iou loss:0.796 avg acc: 0.995 avg f1: 0.436 avg iou: 0.279\n",
      "64  avg iou loss:0.799 avg acc: 0.995 avg f1: 0.428 avg iou: 0.273\n",
      "65  avg iou loss:0.801 avg acc: 0.994 avg f1: 0.439 avg iou: 0.281\n",
      "66  avg iou loss:0.798 avg acc: 0.994 avg f1: 0.440 avg iou: 0.284\n",
      "67  avg iou loss:0.803 avg acc: 0.994 avg f1: 0.420 avg iou: 0.266\n",
      "68*  avg iou loss:0.805 avg acc: 0.995 avg f1: 0.451 avg iou: 0.292\n",
      "69  avg iou loss:0.807 avg acc: 0.994 avg f1: 0.432 avg iou: 0.276\n",
      "70  avg iou loss:0.807 avg acc: 0.994 avg f1: 0.438 avg iou: 0.280\n",
      "71  avg iou loss:0.800 avg acc: 0.994 avg f1: 0.440 avg iou: 0.283\n",
      "72  avg iou loss:0.797 avg acc: 0.995 avg f1: 0.444 avg iou: 0.285\n",
      "73  avg iou loss:0.800 avg acc: 0.994 avg f1: 0.438 avg iou: 0.280\n",
      "74  avg iou loss:0.802 avg acc: 0.995 avg f1: 0.444 avg iou: 0.285\n",
      "75  avg iou loss:0.797 avg acc: 0.995 avg f1: 0.441 avg iou: 0.284\n",
      "76  avg iou loss:0.803 avg acc: 0.995 avg f1: 0.444 avg iou: 0.286\n",
      "77  avg iou loss:0.809 avg acc: 0.995 avg f1: 0.434 avg iou: 0.278\n",
      "78  avg iou loss:0.800 avg acc: 0.995 avg f1: 0.434 avg iou: 0.278\n",
      "79  avg iou loss:0.801 avg acc: 0.994 avg f1: 0.438 avg iou: 0.281\n",
      "80  avg iou loss:0.802 avg acc: 0.995 avg f1: 0.440 avg iou: 0.282\n",
      "81  avg iou loss:0.798 avg acc: 0.994 avg f1: 0.434 avg iou: 0.278\n",
      "82  avg iou loss:0.803 avg acc: 0.995 avg f1: 0.438 avg iou: 0.281\n",
      "83  avg iou loss:0.801 avg acc: 0.995 avg f1: 0.440 avg iou: 0.282\n",
      "84  avg iou loss:0.800 avg acc: 0.994 avg f1: 0.413 avg iou: 0.263\n",
      "85*  avg iou loss:0.800 avg acc: 0.995 avg f1: 0.453 avg iou: 0.294\n",
      "86  avg iou loss:0.801 avg acc: 0.994 avg f1: 0.431 avg iou: 0.276\n",
      "87  avg iou loss:0.807 avg acc: 0.994 avg f1: 0.423 avg iou: 0.270\n",
      "88  avg iou loss:0.799 avg acc: 0.995 avg f1: 0.447 avg iou: 0.289\n",
      "89  avg iou loss:0.800 avg acc: 0.994 avg f1: 0.434 avg iou: 0.278\n",
      "90  avg iou loss:0.799 avg acc: 0.994 avg f1: 0.450 avg iou: 0.291\n",
      "91  avg iou loss:0.801 avg acc: 0.994 avg f1: 0.437 avg iou: 0.280\n",
      "92  avg iou loss:0.800 avg acc: 0.994 avg f1: 0.438 avg iou: 0.280\n",
      "93  avg iou loss:0.809 avg acc: 0.995 avg f1: 0.448 avg iou: 0.289\n",
      "94  avg iou loss:0.796 avg acc: 0.994 avg f1: 0.425 avg iou: 0.271\n",
      "95  avg iou loss:0.821 avg acc: 0.995 avg f1: 0.386 avg iou: 0.241\n",
      "96  avg iou loss:0.813 avg acc: 0.994 avg f1: 0.429 avg iou: 0.274\n",
      "97  avg iou loss:0.812 avg acc: 0.995 avg f1: 0.431 avg iou: 0.276\n",
      "98  avg iou loss:0.804 avg acc: 0.995 avg f1: 0.439 avg iou: 0.281\n",
      "99  avg iou loss:0.804 avg acc: 0.994 avg f1: 0.435 avg iou: 0.278\n"
     ]
    }
   ],
   "source": [
    "model = CNN1(num_features=8)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = IoULoss()\n",
    "epochs = 100\n",
    "best_miou = 0\n",
    "for e in range(epochs):\n",
    "    loss = train(model, trainloader, optimizer, criterion)\n",
    "    aa, miou, mf1 = eval(model,valloader)\n",
    "\n",
    "    if miou > best_miou:\n",
    "        best_miou = miou\n",
    "        cfire = inference(model, testloader)\n",
    "        e = str(e)+\"*\"\n",
    "    print(e, \" avg iou loss:{:.3f} avg acc: {:.3f} avg f1: {:.3f} avg iou: {:.3f}\".format(loss, aa, mf1, miou))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Ideas to implement!\n",
    "\n",
    "<ins>**Ensemble learning - voting**<ins>\n",
    "\n",
    "<ins>**Implement hot spot data pipeline**<ins>\n",
    "\n",
    "<ins>**Make better use of temporal data**<ins>\n",
    "\n",
    "<ins>**Get creative!**<ins>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
